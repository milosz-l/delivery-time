{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "# fact table\n",
    "sessions_df = pd.read_json(\"data/sessions.jsonl\", lines=True)\n",
    "\n",
    "# dimension tables\n",
    "deliveries_df = pd.read_json(\"data/deliveries.jsonl\", lines=True)\n",
    "products_df = pd.read_json(\"data/products.jsonl\", lines=True)\n",
    "users_df = pd.read_json(\"data/users.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAKE_PLOTS = True\n",
    "MAKE_PAIRPLOT = True\n",
    "DATE_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n",
    "PRICE_TRESHOLD = 100_000    # for outliers\n",
    "WEIGHT_TRESHOLD = 50        # for outliers\n",
    "NUM_OF_HOURS = 12\n",
    "SEED = 42\n",
    "SHOW_ALL_WARNINGS = False\n",
    "SHOW_ONLY_ONE_WARNING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "if SHOW_ONLY_ONE_WARNING:\n",
    "    warnings.filterwarnings(action='once')\n",
    "elif not SHOW_ALL_WARNINGS:\n",
    "    warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "deliveries_df[\"delivery_timestamp\"] = deliveries_df[\"delivery_timestamp\"].str.split('.', expand=True)[0]\n",
    "\n",
    "# 2.\n",
    "deliveries_df[\"purchase_timestamp\"] = pd.to_datetime(deliveries_df[\"purchase_timestamp\"], format=DATE_FORMAT)\n",
    "deliveries_df[\"delivery_timestamp\"] = pd.to_datetime(deliveries_df[\"delivery_timestamp\"], format=DATE_FORMAT)\n",
    "\n",
    "# 3.\n",
    "deliveries_df[\"time_diff\"] = deliveries_df[\"delivery_timestamp\"] - deliveries_df[\"purchase_timestamp\"]\n",
    "\n",
    "# 4.\n",
    "deliveries_df = deliveries_df[deliveries_df[\"time_diff\"].notna()]\n",
    "\n",
    "# 5.\n",
    "# time diff as duration in seconds\n",
    "deliveries_df[\"time_diff\"] = deliveries_df[\"time_diff\"].apply(datetime.timedelta.total_seconds)\n",
    "\n",
    "# 6.\n",
    "# deliveries_df = deliveries_df[deliveries_df[\"time_diff\"] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where event_type is not equal \"BUY_PRODUCT\"\n",
    "sessions_df = sessions_df[sessions_df[\"event_type\"] == \"BUY_PRODUCT\"]\n",
    "df = deliveries_df.merge(sessions_df, on=\"purchase_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure, that timestamp == purchase_timestamp\n",
    "num_of_rows_before = df.shape[0]\n",
    "df = df[df[\"timestamp\"] == df[\"purchase_timestamp\"]]\n",
    "num_of_rows_after = df.shape[0]\n",
    "\n",
    "assert(num_of_rows_before == num_of_rows_after)\n",
    "\n",
    "# now we can drop timestamp column, as it is redundant\n",
    "df = df.drop(columns=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(users_df, on=\"user_id\", how=\"left\")\n",
    "df = df.merge(products_df, on=\"product_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejecting outliers for given PRICE_TRESHOLD\n",
    "df = df[df[\"price\"] <= PRICE_TRESHOLD]\n",
    "\n",
    "# rejecting outliers for given WEIGHT_TRESHOLD\n",
    "df = df[df[\"weight_kg\"] <= WEIGHT_TRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting rows with prices below 0\n",
    "df = df[df[\"price\"] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_time_diff_below_0 = df\n",
    "df = df[df[\"time_diff\"] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_of_week'] = df['purchase_timestamp'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_and_street</th>\n",
       "      <th>city</th>\n",
       "      <th>street</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Poznań plac Dębowa 11/53</td>\n",
       "      <td>Poznań</td>\n",
       "      <td>plac Dębowa 11/53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Poznań plac Dębowa 11/53</td>\n",
       "      <td>Poznań</td>\n",
       "      <td>plac Dębowa 11/53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Poznań plac Dębowa 11/53</td>\n",
       "      <td>Poznań</td>\n",
       "      <td>plac Dębowa 11/53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poznań plac Dębowa 11/53</td>\n",
       "      <td>Poznań</td>\n",
       "      <td>plac Dębowa 11/53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Poznań plac Dębowa 11/53</td>\n",
       "      <td>Poznań</td>\n",
       "      <td>plac Dębowa 11/53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11447</th>\n",
       "      <td>Poznań ul. Zachodnia 88</td>\n",
       "      <td>Poznań</td>\n",
       "      <td>ul. Zachodnia 88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11448</th>\n",
       "      <td>Poznań ul. Zachodnia 88</td>\n",
       "      <td>Poznań</td>\n",
       "      <td>ul. Zachodnia 88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11449</th>\n",
       "      <td>Poznań ul. Zachodnia 88</td>\n",
       "      <td>Poznań</td>\n",
       "      <td>ul. Zachodnia 88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11450</th>\n",
       "      <td>Poznań ul. Zachodnia 88</td>\n",
       "      <td>Poznań</td>\n",
       "      <td>ul. Zachodnia 88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11451</th>\n",
       "      <td>Poznań ul. Zachodnia 88</td>\n",
       "      <td>Poznań</td>\n",
       "      <td>ul. Zachodnia 88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11315 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                city_and_street    city             street\n",
       "0      Poznań plac Dębowa 11/53  Poznań  plac Dębowa 11/53\n",
       "1      Poznań plac Dębowa 11/53  Poznań  plac Dębowa 11/53\n",
       "2      Poznań plac Dębowa 11/53  Poznań  plac Dębowa 11/53\n",
       "3      Poznań plac Dębowa 11/53  Poznań  plac Dębowa 11/53\n",
       "4      Poznań plac Dębowa 11/53  Poznań  plac Dębowa 11/53\n",
       "...                         ...     ...                ...\n",
       "11447   Poznań ul. Zachodnia 88  Poznań   ul. Zachodnia 88\n",
       "11448   Poznań ul. Zachodnia 88  Poznań   ul. Zachodnia 88\n",
       "11449   Poznań ul. Zachodnia 88  Poznań   ul. Zachodnia 88\n",
       "11450   Poznań ul. Zachodnia 88  Poznań   ul. Zachodnia 88\n",
       "11451   Poznań ul. Zachodnia 88  Poznań   ul. Zachodnia 88\n",
       "\n",
       "[11315 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['city_and_street'] = df['city'] + ' ' + df['street']\n",
    "display(df[['city_and_street', 'city', 'street']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['purchase_datetime_delta'] = (df['purchase_timestamp'] - df['purchase_timestamp'].min())  / np.timedelta64(1,'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "ADDITIONAL_COLUMNS_TO_DROP = [\"delivery_timestamp\",\n",
    "                              \"session_id\",\n",
    "                              \"purchase_id\",\n",
    "                              \"event_type\",\n",
    "                              \"name\",\n",
    "                              \"city_and_street\",\n",
    "                              \"brand\",\n",
    "                              \"user_id\",\n",
    "                              'product_name',\n",
    "                              'offered_discount']\n",
    "df = df.drop(columns=ADDITIONAL_COLUMNS_TO_DROP)\n",
    "df = df.drop(columns=\"optional_attributes\") # chyba do zmiany - wysokosc itp.\n",
    "df = df.drop(columns=\"purchase_timestamp\") # na pewno do zmiany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_a_col_in_pd(df, col_name):\n",
    "    one_hot = pd.get_dummies(df[col_name], drop_first=True)\n",
    "    df = df.drop(columns=col_name)\n",
    "    df = df.join(one_hot)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_ONE_HOT = [\"delivery_company\", \"city\", \"category_path\", \"street\", 'day_of_week', 'product_id']\n",
    "\n",
    "for col_name in COLUMNS_TO_ONE_HOT:\n",
    "    df = one_hot_encode_a_col_in_pd(df, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11315, 595)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "# one-hot encoding took care of missing data, so shape has not changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11315 entries, 0 to 11451\n",
      "Columns: 595 entries, time_diff to 1653\n",
      "dtypes: float64(4), uint8(591)\n",
      "memory usage: 7.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify columns for standardization scaling (Z-score normalization)\n",
    "cols_to_std = []\n",
    "\n",
    "# specify columns for min-max scaling\n",
    "# offered_discount, price, weight_kg, purchase_datetime_delta\n",
    "cols_to_min_max = ['price', 'weight_kg', 'purchase_datetime_delta']\n",
    "# cols_to_min_max = ['weight_kg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "for col in cols_to_std:\n",
    "    x = df[col].values\n",
    "    std_scaler = StandardScaler()\n",
    "    x_scaled = std_scaler.fit_transform(x.reshape(-1, 1))\n",
    "    df[col] = x_scaled\n",
    "\n",
    "for col in cols_to_min_max:\n",
    "    x = df[col].values\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x.reshape(-1, 1))\n",
    "    df[col] = x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_diff</th>\n",
       "      <th>price</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>purchase_datetime_delta</th>\n",
       "      <th>516</th>\n",
       "      <th>620</th>\n",
       "      <th>Kraków</th>\n",
       "      <th>Poznań</th>\n",
       "      <th>Radom</th>\n",
       "      <th>Szczecin</th>\n",
       "      <th>...</th>\n",
       "      <th>1627</th>\n",
       "      <th>1628</th>\n",
       "      <th>1629</th>\n",
       "      <th>1630</th>\n",
       "      <th>1631</th>\n",
       "      <th>1632</th>\n",
       "      <th>1633</th>\n",
       "      <th>1634</th>\n",
       "      <th>1635</th>\n",
       "      <th>1653</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194397.0</td>\n",
       "      <td>0.228930</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.459215</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184285.0</td>\n",
       "      <td>0.406238</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.191245</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>164705.0</td>\n",
       "      <td>0.665416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>252885.0</td>\n",
       "      <td>0.448400</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.822722</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228672.0</td>\n",
       "      <td>0.032173</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 595 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_diff     price  weight_kg  purchase_datetime_delta  516  620  Kraków  \\\n",
       "0   194397.0  0.228930   0.020000                 0.459215    0    1       0   \n",
       "1   184285.0  0.406238   0.000800                 0.191245    0    1       0   \n",
       "2   164705.0  0.665416   0.000000                 0.082285    0    0       0   \n",
       "3   252885.0  0.448400   0.010667                 0.822722    0    1       0   \n",
       "4   228672.0  0.032173   0.008667                 0.343154    1    0       0   \n",
       "\n",
       "   Poznań  Radom  Szczecin  ...  1627  1628  1629  1630  1631  1632  1633  \\\n",
       "0       1      0         0  ...     0     0     0     0     0     0     0   \n",
       "1       1      0         0  ...     0     0     0     0     0     0     0   \n",
       "2       1      0         0  ...     0     0     0     0     0     0     0   \n",
       "3       1      0         0  ...     0     0     0     0     0     0     0   \n",
       "4       1      0         0  ...     0     0     0     0     0     0     0   \n",
       "\n",
       "   1634  1635  1653  \n",
       "0     0     0     0  \n",
       "1     0     0     0  \n",
       "2     0     0     0  \n",
       "3     0     0     0  \n",
       "4     0     0     0  \n",
       "\n",
       "[5 rows x 595 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def split_data(df, target_column=\"time_diff\"):\n",
    "    y = df[\"time_diff\"].to_numpy()\n",
    "    X = df.drop(columns=\"time_diff\")\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(models_list, X_train, y_train):\n",
    "    for model in models_list:\n",
    "        model.fit(X_train, y_train)\n",
    "    return models_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_with_predictions(models_list, X_test, y_test):\n",
    "    y_pred_df = pd.DataFrame()\n",
    "    y_pred_df[\"y_test\"] = y_test\n",
    "    for model in models_list:\n",
    "        y_pred_df[f\"{type(model).__name__} prediction\"] = model.predict(X_test)\n",
    "    return y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_predictions(y_pred_df):\n",
    "    display(y_pred_df.head())\n",
    "    display(y_pred_df.info())\n",
    "    display(y_pred_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(models_list, X_test, y_test):\n",
    "    for model in models_list:\n",
    "        score = model.score(X_test, y_test)\n",
    "        print(f\"{type(model).__name__} score = {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_percent_of_good_predictions(models_list, X_test, y_test, error=NUM_OF_HOURS*60*60):\n",
    "    for model in models_list:\n",
    "        predictions = model.predict(X_test)\n",
    "        predictions_time_diff = np.abs(y_test - predictions)\n",
    "        num_of_good_predictions = (predictions_time_diff < error).sum()\n",
    "        percent_of_good_predictions = num_of_good_predictions / len(predictions_time_diff)\n",
    "        print(f'number of good predictions for {type(model).__name__} = {num_of_good_predictions}')\n",
    "        print(f'which is {percent_of_good_predictions * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = split_data(df)\n",
    "\n",
    "# models_list = [Ridge(alpha=0.1),\n",
    "#                Lasso(alpha=0.1),\n",
    "#                DecisionTreeRegressor(random_state=SEED),\n",
    "#                RandomForestRegressor(random_state=SEED)]\n",
    "# models_list = train_models(models_list, X_train, y_train)\n",
    "\n",
    "# y_pred_df = create_df_with_predictions(models_list, X_test, y_test)\n",
    "# # display_predictions(y_pred_df)\n",
    "\n",
    "# print_scores(models_list, X_test, y_test)\n",
    "\n",
    "# print_percent_of_good_predictions(models_list, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(df)\n",
    "input_shape = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "36/36 [==============================] - 2s 34ms/step - loss: 233531.7969 - mae: 233531.7969 - val_loss: 231076.8906 - val_mae: 231076.8906\n",
      "Epoch 2/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 233530.0312 - mae: 233530.0312 - val_loss: 231075.3438 - val_mae: 231075.3438\n",
      "Epoch 3/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 233526.2500 - mae: 233526.2500 - val_loss: 231072.0625 - val_mae: 231072.0625\n",
      "Epoch 4/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 233519.1094 - mae: 233519.1094 - val_loss: 231066.0312 - val_mae: 231066.0312\n",
      "Epoch 5/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 233508.3281 - mae: 233508.3281 - val_loss: 231056.9531 - val_mae: 231056.9531\n",
      "Epoch 6/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 233493.5000 - mae: 233493.5000 - val_loss: 231044.6562 - val_mae: 231044.6562\n",
      "Epoch 7/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 233475.0156 - mae: 233475.0156 - val_loss: 231027.7344 - val_mae: 231027.7344\n",
      "Epoch 8/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 233452.4688 - mae: 233452.4688 - val_loss: 231005.4844 - val_mae: 231005.4844\n",
      "Epoch 9/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 233426.2344 - mae: 233426.2344 - val_loss: 230979.3438 - val_mae: 230979.3438\n",
      "Epoch 10/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 233396.1719 - mae: 233396.1719 - val_loss: 230948.5625 - val_mae: 230948.5625\n",
      "Epoch 11/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 233362.7656 - mae: 233362.7656 - val_loss: 230910.5938 - val_mae: 230910.5938\n",
      "Epoch 12/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 233325.6875 - mae: 233325.6875 - val_loss: 230867.4531 - val_mae: 230867.4531\n",
      "Epoch 13/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 233285.2656 - mae: 233285.2656 - val_loss: 230820.7656 - val_mae: 230820.7656\n",
      "Epoch 14/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 233241.2500 - mae: 233241.2500 - val_loss: 230771.2812 - val_mae: 230771.2812\n",
      "Epoch 15/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 233193.6406 - mae: 233193.6406 - val_loss: 230717.2344 - val_mae: 230717.2344\n",
      "Epoch 16/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 233142.7812 - mae: 233142.7812 - val_loss: 230658.5469 - val_mae: 230658.5469\n",
      "Epoch 17/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 233089.2500 - mae: 233089.2500 - val_loss: 230598.7656 - val_mae: 230598.7656\n",
      "Epoch 18/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 233031.9375 - mae: 233031.9375 - val_loss: 230536.2969 - val_mae: 230536.2969\n",
      "Epoch 19/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 232971.8594 - mae: 232971.8594 - val_loss: 230470.9531 - val_mae: 230470.9531\n",
      "Epoch 20/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 232908.3438 - mae: 232908.3438 - val_loss: 230402.7500 - val_mae: 230402.7500\n",
      "Epoch 21/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 232842.2812 - mae: 232842.2812 - val_loss: 230332.7031 - val_mae: 230332.7031\n",
      "Epoch 22/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 232773.0000 - mae: 232773.0000 - val_loss: 230260.0625 - val_mae: 230260.0625\n",
      "Epoch 23/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 232700.9688 - mae: 232700.9688 - val_loss: 230184.0312 - val_mae: 230184.0312\n",
      "Epoch 24/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 232625.4062 - mae: 232625.4062 - val_loss: 230106.7500 - val_mae: 230106.7500\n",
      "Epoch 25/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 232547.4219 - mae: 232547.4219 - val_loss: 230027.1406 - val_mae: 230027.1406\n",
      "Epoch 26/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 232466.8281 - mae: 232466.8281 - val_loss: 229944.4062 - val_mae: 229944.4062\n",
      "Epoch 27/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 232382.8594 - mae: 232382.8594 - val_loss: 229858.2188 - val_mae: 229858.2188\n",
      "Epoch 28/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 232295.6719 - mae: 232295.6719 - val_loss: 229767.6719 - val_mae: 229767.6719\n",
      "Epoch 29/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 232206.1406 - mae: 232206.1406 - val_loss: 229676.4062 - val_mae: 229676.4062\n",
      "Epoch 30/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 232113.9531 - mae: 232113.9531 - val_loss: 229581.4062 - val_mae: 229581.4062\n",
      "Epoch 31/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 232019.6562 - mae: 232019.6562 - val_loss: 229484.3281 - val_mae: 229484.3281\n",
      "Epoch 32/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 231922.3594 - mae: 231922.3594 - val_loss: 229384.5938 - val_mae: 229384.5938\n",
      "Epoch 33/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 231822.8750 - mae: 231822.8750 - val_loss: 229284.5938 - val_mae: 229284.5938\n",
      "Epoch 34/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 231719.4688 - mae: 231719.4688 - val_loss: 229181.3125 - val_mae: 229181.3125\n",
      "Epoch 35/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 231614.3594 - mae: 231614.3594 - val_loss: 229071.4531 - val_mae: 229071.4531\n",
      "Epoch 36/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 231507.0781 - mae: 231507.0781 - val_loss: 228960.5781 - val_mae: 228960.5781\n",
      "Epoch 37/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 231396.2656 - mae: 231396.2656 - val_loss: 228846.2969 - val_mae: 228846.2969\n",
      "Epoch 38/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 231283.8594 - mae: 231283.8594 - val_loss: 228733.1719 - val_mae: 228733.1719\n",
      "Epoch 39/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 231168.5938 - mae: 231168.5938 - val_loss: 228617.3750 - val_mae: 228617.3750\n",
      "Epoch 40/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 231052.4219 - mae: 231052.4219 - val_loss: 228500.5469 - val_mae: 228500.5469\n",
      "Epoch 41/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 230931.9531 - mae: 230931.9531 - val_loss: 228380.3594 - val_mae: 228380.3594\n",
      "Epoch 42/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 230809.3125 - mae: 230809.3125 - val_loss: 228254.5156 - val_mae: 228254.5156\n",
      "Epoch 43/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 230684.4219 - mae: 230684.4219 - val_loss: 228127.3906 - val_mae: 228127.3906\n",
      "Epoch 44/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 230557.4844 - mae: 230557.4844 - val_loss: 227999.7812 - val_mae: 227999.7812\n",
      "Epoch 45/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 230428.7812 - mae: 230428.7812 - val_loss: 227870.5938 - val_mae: 227870.5938\n",
      "Epoch 46/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 230295.3594 - mae: 230295.3594 - val_loss: 227735.7656 - val_mae: 227735.7656\n",
      "Epoch 47/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 230163.6250 - mae: 230163.6250 - val_loss: 227600.1562 - val_mae: 227600.1562\n",
      "Epoch 48/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 230025.9844 - mae: 230025.9844 - val_loss: 227462.7656 - val_mae: 227462.7656\n",
      "Epoch 49/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 229887.8438 - mae: 229887.8438 - val_loss: 227324.7031 - val_mae: 227324.7031\n",
      "Epoch 50/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 229746.9688 - mae: 229746.9688 - val_loss: 227183.0625 - val_mae: 227183.0625\n",
      "Epoch 51/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 229601.8281 - mae: 229601.8281 - val_loss: 227041.2812 - val_mae: 227041.2812\n",
      "Epoch 52/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 229462.5469 - mae: 229462.5469 - val_loss: 226893.9375 - val_mae: 226893.9375\n",
      "Epoch 53/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 229313.4062 - mae: 229313.4062 - val_loss: 226745.2969 - val_mae: 226745.2969\n",
      "Epoch 54/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 229164.1562 - mae: 229164.1562 - val_loss: 226594.0938 - val_mae: 226594.0938\n",
      "Epoch 55/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 229013.2500 - mae: 229013.2500 - val_loss: 226439.2969 - val_mae: 226439.2969\n",
      "Epoch 56/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 228859.8594 - mae: 228859.8594 - val_loss: 226282.7812 - val_mae: 226282.7812\n",
      "Epoch 57/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 228702.7812 - mae: 228702.7812 - val_loss: 226123.4062 - val_mae: 226123.4062\n",
      "Epoch 58/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 228548.3906 - mae: 228548.3906 - val_loss: 225964.4531 - val_mae: 225964.4531\n",
      "Epoch 59/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 228387.2188 - mae: 228387.2188 - val_loss: 225805.7656 - val_mae: 225805.7656\n",
      "Epoch 60/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 228222.2500 - mae: 228222.2500 - val_loss: 225640.3750 - val_mae: 225640.3750\n",
      "Epoch 61/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 228060.7656 - mae: 228060.7656 - val_loss: 225472.5156 - val_mae: 225472.5156\n",
      "Epoch 62/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 227896.5469 - mae: 227896.5469 - val_loss: 225306.7500 - val_mae: 225306.7500\n",
      "Epoch 63/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 227725.3438 - mae: 227725.3438 - val_loss: 225140.8125 - val_mae: 225140.8125\n",
      "Epoch 64/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 227557.4062 - mae: 227557.4062 - val_loss: 224968.2031 - val_mae: 224968.2031\n",
      "Epoch 65/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 227384.3281 - mae: 227384.3281 - val_loss: 224796.3594 - val_mae: 224796.3594\n",
      "Epoch 66/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 227213.0156 - mae: 227213.0156 - val_loss: 224622.7031 - val_mae: 224622.7031\n",
      "Epoch 67/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 227041.1094 - mae: 227041.1094 - val_loss: 224446.3281 - val_mae: 224446.3281\n",
      "Epoch 68/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 226861.7344 - mae: 226861.7344 - val_loss: 224266.7656 - val_mae: 224266.7656\n",
      "Epoch 69/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 226681.6875 - mae: 226681.6875 - val_loss: 224089.0781 - val_mae: 224089.0781\n",
      "Epoch 70/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 226500.8125 - mae: 226500.8125 - val_loss: 223904.8438 - val_mae: 223904.8438\n",
      "Epoch 71/500\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 226316.0938 - mae: 226316.0938 - val_loss: 223717.4062 - val_mae: 223717.4062\n",
      "Epoch 72/500\n",
      "36/36 [==============================] - 1s 41ms/step - loss: 226136.8438 - mae: 226136.8438 - val_loss: 223530.3438 - val_mae: 223530.3438\n",
      "Epoch 73/500\n",
      "36/36 [==============================] - 1s 41ms/step - loss: 225943.9531 - mae: 225943.9531 - val_loss: 223340.3281 - val_mae: 223340.3281\n",
      "Epoch 74/500\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 225758.4531 - mae: 225758.4531 - val_loss: 223150.4531 - val_mae: 223150.4531\n",
      "Epoch 75/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 225566.5938 - mae: 225566.5938 - val_loss: 222962.1406 - val_mae: 222962.1406\n",
      "Epoch 76/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 225373.8438 - mae: 225373.8438 - val_loss: 222771.1250 - val_mae: 222771.1250\n",
      "Epoch 77/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 225182.3125 - mae: 225182.3125 - val_loss: 222576.2969 - val_mae: 222576.2969\n",
      "Epoch 78/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 224988.4375 - mae: 224988.4375 - val_loss: 222381.8594 - val_mae: 222381.8594\n",
      "Epoch 79/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 224779.6406 - mae: 224779.6406 - val_loss: 222181.1719 - val_mae: 222181.1719\n",
      "Epoch 80/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 224585.0000 - mae: 224585.0000 - val_loss: 221980.3906 - val_mae: 221980.3906\n",
      "Epoch 81/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 224383.5312 - mae: 224383.5312 - val_loss: 221770.2812 - val_mae: 221770.2812\n",
      "Epoch 82/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 224180.5000 - mae: 224180.5000 - val_loss: 221565.9062 - val_mae: 221565.9062\n",
      "Epoch 83/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 223977.2031 - mae: 223977.2031 - val_loss: 221358.5000 - val_mae: 221358.5000\n",
      "Epoch 84/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 223765.5000 - mae: 223765.5000 - val_loss: 221152.7500 - val_mae: 221152.7500\n",
      "Epoch 85/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 223559.7969 - mae: 223559.7969 - val_loss: 220946.9375 - val_mae: 220946.9375\n",
      "Epoch 86/500\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 223350.9531 - mae: 223350.9531 - val_loss: 220735.9062 - val_mae: 220735.9062\n",
      "Epoch 87/500\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 223139.6094 - mae: 223139.6094 - val_loss: 220522.5000 - val_mae: 220522.5000\n",
      "Epoch 88/500\n",
      "36/36 [==============================] - 2s 43ms/step - loss: 222919.8281 - mae: 222919.8281 - val_loss: 220304.9375 - val_mae: 220304.9375\n",
      "Epoch 89/500\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 222707.8750 - mae: 222707.8750 - val_loss: 220086.5156 - val_mae: 220086.5156\n",
      "Epoch 90/500\n",
      "36/36 [==============================] - 1s 39ms/step - loss: 222489.5625 - mae: 222489.5625 - val_loss: 219866.3125 - val_mae: 219866.3125\n",
      "Epoch 91/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 222275.6719 - mae: 222275.6719 - val_loss: 219646.2500 - val_mae: 219646.2500\n",
      "Epoch 92/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 222054.8281 - mae: 222054.8281 - val_loss: 219422.5938 - val_mae: 219422.5938\n",
      "Epoch 93/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 221836.2500 - mae: 221836.2500 - val_loss: 219201.4062 - val_mae: 219201.4062\n",
      "Epoch 94/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 221598.2344 - mae: 221598.2344 - val_loss: 218980.3125 - val_mae: 218980.3125\n",
      "Epoch 95/500\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 221381.9688 - mae: 221381.9688 - val_loss: 218754.6250 - val_mae: 218754.6250\n",
      "Epoch 96/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 221148.5938 - mae: 221148.5938 - val_loss: 218520.8438 - val_mae: 218520.8438\n",
      "Epoch 97/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 220920.9062 - mae: 220920.9062 - val_loss: 218287.6719 - val_mae: 218287.6719\n",
      "Epoch 98/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 220685.1719 - mae: 220685.1719 - val_loss: 218054.2656 - val_mae: 218054.2656\n",
      "Epoch 99/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 220462.0469 - mae: 220462.0469 - val_loss: 217820.3750 - val_mae: 217820.3750\n",
      "Epoch 100/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 220225.5156 - mae: 220225.5156 - val_loss: 217586.2344 - val_mae: 217586.2344\n",
      "Epoch 101/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 219987.6875 - mae: 219987.6875 - val_loss: 217353.4219 - val_mae: 217353.4219\n",
      "Epoch 102/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 219750.2969 - mae: 219750.2969 - val_loss: 217112.8750 - val_mae: 217112.8750\n",
      "Epoch 103/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 219509.3438 - mae: 219509.3438 - val_loss: 216872.7031 - val_mae: 216872.7031\n",
      "Epoch 104/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 219268.2344 - mae: 219268.2344 - val_loss: 216627.5781 - val_mae: 216627.5781\n",
      "Epoch 105/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 219030.6250 - mae: 219030.6250 - val_loss: 216381.7031 - val_mae: 216381.7031\n",
      "Epoch 106/500\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 218791.5781 - mae: 218791.5781 - val_loss: 216135.3750 - val_mae: 216135.3750\n",
      "Epoch 107/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 218536.2344 - mae: 218536.2344 - val_loss: 215890.5781 - val_mae: 215890.5781\n",
      "Epoch 108/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 218286.4375 - mae: 218286.4375 - val_loss: 215636.7031 - val_mae: 215636.7031\n",
      "Epoch 109/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 218037.2969 - mae: 218037.2969 - val_loss: 215383.6094 - val_mae: 215383.6094\n",
      "Epoch 110/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 217792.6250 - mae: 217792.6250 - val_loss: 215135.0469 - val_mae: 215135.0469\n",
      "Epoch 111/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 217534.9375 - mae: 217534.9375 - val_loss: 214882.2656 - val_mae: 214882.2656\n",
      "Epoch 112/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 217288.4219 - mae: 217288.4219 - val_loss: 214627.0938 - val_mae: 214627.0938\n",
      "Epoch 113/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 217030.6094 - mae: 217030.6094 - val_loss: 214371.0781 - val_mae: 214371.0781\n",
      "Epoch 114/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 216768.7656 - mae: 216768.7656 - val_loss: 214110.5781 - val_mae: 214110.5781\n",
      "Epoch 115/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 216517.5781 - mae: 216517.5781 - val_loss: 213851.5156 - val_mae: 213851.5156\n",
      "Epoch 116/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 216260.1094 - mae: 216260.1094 - val_loss: 213593.5938 - val_mae: 213593.5938\n",
      "Epoch 117/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 215994.9219 - mae: 215994.9219 - val_loss: 213330.9062 - val_mae: 213330.9062\n",
      "Epoch 118/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 215724.4688 - mae: 215724.4688 - val_loss: 213065.0000 - val_mae: 213065.0000\n",
      "Epoch 119/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 215463.2344 - mae: 215463.2344 - val_loss: 212800.3438 - val_mae: 212800.3438\n",
      "Epoch 120/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 215190.7500 - mae: 215190.7500 - val_loss: 212524.6406 - val_mae: 212524.6406\n",
      "Epoch 121/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 214923.7031 - mae: 214923.7031 - val_loss: 212247.8281 - val_mae: 212247.8281\n",
      "Epoch 122/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 214662.4531 - mae: 214662.4531 - val_loss: 211979.3906 - val_mae: 211979.3906\n",
      "Epoch 123/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 214388.5156 - mae: 214388.5156 - val_loss: 211708.8906 - val_mae: 211708.8906\n",
      "Epoch 124/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 214122.4688 - mae: 214122.4688 - val_loss: 211429.9844 - val_mae: 211429.9844\n",
      "Epoch 125/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 213839.2500 - mae: 213839.2500 - val_loss: 211153.2031 - val_mae: 211153.2031\n",
      "Epoch 126/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 213568.7188 - mae: 213568.7188 - val_loss: 210880.5469 - val_mae: 210880.5469\n",
      "Epoch 127/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 213290.0781 - mae: 213290.0781 - val_loss: 210607.5156 - val_mae: 210607.5156\n",
      "Epoch 128/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 213011.2969 - mae: 213011.2969 - val_loss: 210323.6562 - val_mae: 210323.6562\n",
      "Epoch 129/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 212726.6250 - mae: 212726.6250 - val_loss: 210041.7969 - val_mae: 210041.7969\n",
      "Epoch 130/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 212448.5156 - mae: 212448.5156 - val_loss: 209758.9531 - val_mae: 209758.9531\n",
      "Epoch 131/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 212155.3906 - mae: 212155.3906 - val_loss: 209472.8125 - val_mae: 209472.8125\n",
      "Epoch 132/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 211877.7656 - mae: 211877.7656 - val_loss: 209184.0781 - val_mae: 209184.0781\n",
      "Epoch 133/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 211586.8750 - mae: 211586.8750 - val_loss: 208896.4844 - val_mae: 208896.4844\n",
      "Epoch 134/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 211297.7969 - mae: 211297.7969 - val_loss: 208601.7188 - val_mae: 208601.7188\n",
      "Epoch 135/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 211009.4062 - mae: 211009.4062 - val_loss: 208312.9375 - val_mae: 208312.9375\n",
      "Epoch 136/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 210732.2812 - mae: 210732.2812 - val_loss: 208016.2812 - val_mae: 208016.2812\n",
      "Epoch 137/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 210420.9375 - mae: 210420.9375 - val_loss: 207725.3750 - val_mae: 207725.3750\n",
      "Epoch 138/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 210118.2031 - mae: 210118.2031 - val_loss: 207427.5000 - val_mae: 207427.5000\n",
      "Epoch 139/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 209833.4531 - mae: 209833.4531 - val_loss: 207131.1719 - val_mae: 207131.1719\n",
      "Epoch 140/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 209546.9844 - mae: 209546.9844 - val_loss: 206825.5156 - val_mae: 206825.5156\n",
      "Epoch 141/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 209241.8281 - mae: 209241.8281 - val_loss: 206530.3438 - val_mae: 206530.3438\n",
      "Epoch 142/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 208930.6094 - mae: 208930.6094 - val_loss: 206229.7031 - val_mae: 206229.7031\n",
      "Epoch 143/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 208636.7344 - mae: 208636.7344 - val_loss: 205925.0625 - val_mae: 205925.0625\n",
      "Epoch 144/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 208333.7031 - mae: 208333.7031 - val_loss: 205619.5938 - val_mae: 205619.5938\n",
      "Epoch 145/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 208031.4062 - mae: 208031.4062 - val_loss: 205315.2969 - val_mae: 205315.2969\n",
      "Epoch 146/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 207729.1406 - mae: 207729.1406 - val_loss: 205006.1094 - val_mae: 205006.1094\n",
      "Epoch 147/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 207420.9844 - mae: 207420.9844 - val_loss: 204692.3125 - val_mae: 204692.3125\n",
      "Epoch 148/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 207099.0938 - mae: 207099.0938 - val_loss: 204377.4531 - val_mae: 204377.4531\n",
      "Epoch 149/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 206799.6562 - mae: 206799.6562 - val_loss: 204061.7344 - val_mae: 204061.7344\n",
      "Epoch 150/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 206487.2656 - mae: 206487.2656 - val_loss: 203753.0156 - val_mae: 203753.0156\n",
      "Epoch 151/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 206167.4531 - mae: 206167.4531 - val_loss: 203440.4844 - val_mae: 203440.4844\n",
      "Epoch 152/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 205857.5156 - mae: 205857.5156 - val_loss: 203126.4062 - val_mae: 203126.4062\n",
      "Epoch 153/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 205533.1250 - mae: 205533.1250 - val_loss: 202806.8594 - val_mae: 202806.8594\n",
      "Epoch 154/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 205214.7656 - mae: 205214.7656 - val_loss: 202487.2188 - val_mae: 202487.2188\n",
      "Epoch 155/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 204881.0156 - mae: 204881.0156 - val_loss: 202160.2500 - val_mae: 202160.2500\n",
      "Epoch 156/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 204570.2656 - mae: 204570.2656 - val_loss: 201840.2188 - val_mae: 201840.2188\n",
      "Epoch 157/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 204235.6875 - mae: 204235.6875 - val_loss: 201514.6406 - val_mae: 201514.6406\n",
      "Epoch 158/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 203925.4688 - mae: 203925.4688 - val_loss: 201185.6250 - val_mae: 201185.6250\n",
      "Epoch 159/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 203599.3281 - mae: 203599.3281 - val_loss: 200855.6875 - val_mae: 200855.6875\n",
      "Epoch 160/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 203283.5156 - mae: 203283.5156 - val_loss: 200527.7031 - val_mae: 200527.7031\n",
      "Epoch 161/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 202941.2500 - mae: 202941.2500 - val_loss: 200203.2344 - val_mae: 200203.2344\n",
      "Epoch 162/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 202612.0938 - mae: 202612.0938 - val_loss: 199867.7812 - val_mae: 199867.7812\n",
      "Epoch 163/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 202264.9375 - mae: 202264.9375 - val_loss: 199529.6250 - val_mae: 199529.6250\n",
      "Epoch 164/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 201950.0625 - mae: 201950.0625 - val_loss: 199195.8125 - val_mae: 199195.8125\n",
      "Epoch 165/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 201591.1250 - mae: 201591.1250 - val_loss: 198858.8125 - val_mae: 198858.8125\n",
      "Epoch 166/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 201276.0156 - mae: 201276.0156 - val_loss: 198528.1406 - val_mae: 198528.1406\n",
      "Epoch 167/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 200937.0156 - mae: 200937.0156 - val_loss: 198189.5312 - val_mae: 198189.5312\n",
      "Epoch 168/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 200594.6875 - mae: 200594.6875 - val_loss: 197848.6562 - val_mae: 197848.6562\n",
      "Epoch 169/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 200229.5625 - mae: 200229.5625 - val_loss: 197503.0625 - val_mae: 197503.0625\n",
      "Epoch 170/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 199904.5000 - mae: 199904.5000 - val_loss: 197159.5156 - val_mae: 197159.5156\n",
      "Epoch 171/500\n",
      "36/36 [==============================] - 1s 41ms/step - loss: 199568.9531 - mae: 199568.9531 - val_loss: 196812.4219 - val_mae: 196812.4219\n",
      "Epoch 172/500\n",
      "36/36 [==============================] - 1s 41ms/step - loss: 199206.4062 - mae: 199206.4062 - val_loss: 196465.8750 - val_mae: 196465.8750\n",
      "Epoch 173/500\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 198899.9844 - mae: 198899.9844 - val_loss: 196114.8750 - val_mae: 196114.8750\n",
      "Epoch 174/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 198535.1562 - mae: 198535.1562 - val_loss: 195760.6719 - val_mae: 195760.6719\n",
      "Epoch 175/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 198170.7812 - mae: 198170.7812 - val_loss: 195411.3594 - val_mae: 195411.3594\n",
      "Epoch 176/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 197804.3125 - mae: 197804.3125 - val_loss: 195063.0000 - val_mae: 195063.0000\n",
      "Epoch 177/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 197467.0156 - mae: 197467.0156 - val_loss: 194709.4219 - val_mae: 194709.4219\n",
      "Epoch 178/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 197125.8125 - mae: 197125.8125 - val_loss: 194348.4688 - val_mae: 194348.4688\n",
      "Epoch 179/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 196743.9688 - mae: 196743.9688 - val_loss: 193991.2500 - val_mae: 193991.2500\n",
      "Epoch 180/500\n",
      "36/36 [==============================] - 1s 41ms/step - loss: 196411.9844 - mae: 196411.9844 - val_loss: 193627.1406 - val_mae: 193627.1406\n",
      "Epoch 181/500\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 196048.4219 - mae: 196048.4219 - val_loss: 193269.6875 - val_mae: 193269.6875\n",
      "Epoch 182/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 195682.3125 - mae: 195682.3125 - val_loss: 192909.1406 - val_mae: 192909.1406\n",
      "Epoch 183/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 195333.1406 - mae: 195333.1406 - val_loss: 192540.0625 - val_mae: 192540.0625\n",
      "Epoch 184/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 194959.4062 - mae: 194959.4062 - val_loss: 192181.8750 - val_mae: 192181.8750\n",
      "Epoch 185/500\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 194607.2969 - mae: 194607.2969 - val_loss: 191814.0312 - val_mae: 191814.0312\n",
      "Epoch 186/500\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 194245.3438 - mae: 194245.3438 - val_loss: 191447.4062 - val_mae: 191447.4062\n",
      "Epoch 187/500\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 193880.3906 - mae: 193880.3906 - val_loss: 191078.3281 - val_mae: 191078.3281\n",
      "Epoch 188/500\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 193476.8438 - mae: 193476.8438 - val_loss: 190693.6875 - val_mae: 190693.6875\n",
      "Epoch 189/500\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 193110.8281 - mae: 193110.8281 - val_loss: 190320.4531 - val_mae: 190320.4531\n",
      "Epoch 190/500\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 192735.2188 - mae: 192735.2188 - val_loss: 189948.1094 - val_mae: 189948.1094\n",
      "Epoch 191/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 192383.3125 - mae: 192383.3125 - val_loss: 189585.2969 - val_mae: 189585.2969\n",
      "Epoch 192/500\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 192009.1094 - mae: 192009.1094 - val_loss: 189219.0156 - val_mae: 189219.0156\n",
      "Epoch 193/500\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 191643.1250 - mae: 191643.1250 - val_loss: 188836.9688 - val_mae: 188836.9688\n",
      "Epoch 194/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 191253.1875 - mae: 191253.1875 - val_loss: 188458.6562 - val_mae: 188458.6562\n",
      "Epoch 195/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 190873.5625 - mae: 190873.5625 - val_loss: 188072.0469 - val_mae: 188072.0469\n",
      "Epoch 196/500\n",
      "36/36 [==============================] - 1s 39ms/step - loss: 190491.8594 - mae: 190491.8594 - val_loss: 187671.5469 - val_mae: 187671.5469\n",
      "Epoch 197/500\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 190098.2344 - mae: 190098.2344 - val_loss: 187287.8594 - val_mae: 187287.8594\n",
      "Epoch 198/500\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 189705.1562 - mae: 189705.1562 - val_loss: 186916.6250 - val_mae: 186916.6250\n",
      "Epoch 199/500\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 189335.6719 - mae: 189335.6719 - val_loss: 186519.9844 - val_mae: 186519.9844\n",
      "Epoch 200/500\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 188959.1094 - mae: 188959.1094 - val_loss: 186129.3438 - val_mae: 186129.3438\n",
      "Epoch 201/500\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 188561.9375 - mae: 188561.9375 - val_loss: 185731.7344 - val_mae: 185731.7344\n",
      "Epoch 202/500\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 188190.6719 - mae: 188190.6719 - val_loss: 185342.7969 - val_mae: 185342.7969\n",
      "Epoch 203/500\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 187777.4688 - mae: 187777.4688 - val_loss: 184935.9844 - val_mae: 184935.9844\n",
      "Epoch 204/500\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 187395.2812 - mae: 187395.2812 - val_loss: 184530.2500 - val_mae: 184530.2500\n",
      "Epoch 205/500\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 186985.1094 - mae: 186985.1094 - val_loss: 184102.5000 - val_mae: 184102.5000\n",
      "Epoch 206/500\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 186597.5625 - mae: 186597.5625 - val_loss: 183694.2188 - val_mae: 183694.2188\n",
      "Epoch 207/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 186201.9688 - mae: 186201.9688 - val_loss: 183281.2031 - val_mae: 183281.2031\n",
      "Epoch 208/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 185831.5156 - mae: 185831.5156 - val_loss: 182871.3594 - val_mae: 182871.3594\n",
      "Epoch 209/500\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 185415.7969 - mae: 185415.7969 - val_loss: 182467.7969 - val_mae: 182467.7969\n",
      "Epoch 210/500\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 184995.2031 - mae: 184995.2031 - val_loss: 182055.0781 - val_mae: 182055.0781\n",
      "Epoch 211/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 184589.2344 - mae: 184589.2344 - val_loss: 181626.3750 - val_mae: 181626.3750\n",
      "Epoch 212/500\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 184225.9531 - mae: 184225.9531 - val_loss: 181207.0469 - val_mae: 181207.0469\n",
      "Epoch 213/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 183802.1875 - mae: 183802.1875 - val_loss: 180842.2031 - val_mae: 180842.2031\n",
      "Epoch 214/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 183422.5938 - mae: 183422.5938 - val_loss: 180393.9844 - val_mae: 180393.9844\n",
      "Epoch 215/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 182967.8438 - mae: 182967.8438 - val_loss: 180052.3594 - val_mae: 180052.3594\n",
      "Epoch 216/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 182591.1875 - mae: 182591.1875 - val_loss: 179719.7656 - val_mae: 179719.7656\n",
      "Epoch 217/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 182164.5000 - mae: 182164.5000 - val_loss: 179207.3594 - val_mae: 179207.3594\n",
      "Epoch 218/500\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 181769.5312 - mae: 181769.5312 - val_loss: 178786.5469 - val_mae: 178786.5469\n",
      "Epoch 219/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 181339.1562 - mae: 181339.1562 - val_loss: 178467.3594 - val_mae: 178467.3594\n",
      "Epoch 220/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 180901.5000 - mae: 180901.5000 - val_loss: 178067.4375 - val_mae: 178067.4375\n",
      "Epoch 221/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 180498.3906 - mae: 180498.3906 - val_loss: 177640.1562 - val_mae: 177640.1562\n",
      "Epoch 222/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 180091.7188 - mae: 180091.7188 - val_loss: 177134.4844 - val_mae: 177134.4844\n",
      "Epoch 223/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 179669.9531 - mae: 179669.9531 - val_loss: 176723.7969 - val_mae: 176723.7969\n",
      "Epoch 224/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 179278.1719 - mae: 179278.1719 - val_loss: 176268.1406 - val_mae: 176268.1406\n",
      "Epoch 225/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 178815.6875 - mae: 178815.6875 - val_loss: 175860.2656 - val_mae: 175860.2656\n",
      "Epoch 226/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 178398.1406 - mae: 178398.1406 - val_loss: 175431.9375 - val_mae: 175431.9375\n",
      "Epoch 227/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 177935.8125 - mae: 177935.8125 - val_loss: 175041.0312 - val_mae: 175041.0312\n",
      "Epoch 228/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 177503.5781 - mae: 177503.5781 - val_loss: 174625.9375 - val_mae: 174625.9375\n",
      "Epoch 229/500\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 177101.2656 - mae: 177101.2656 - val_loss: 174248.7188 - val_mae: 174248.7188\n",
      "Epoch 230/500\n",
      "36/36 [==============================] - 2s 45ms/step - loss: 176674.5469 - mae: 176674.5469 - val_loss: 173755.0781 - val_mae: 173755.0781\n",
      "Epoch 231/500\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 176255.1719 - mae: 176255.1719 - val_loss: 173382.1250 - val_mae: 173382.1250\n",
      "Epoch 232/500\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 175794.3594 - mae: 175794.3594 - val_loss: 172978.4375 - val_mae: 172978.4375\n",
      "Epoch 233/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 175374.2812 - mae: 175374.2812 - val_loss: 172567.3438 - val_mae: 172567.3438\n",
      "Epoch 234/500\n",
      "36/36 [==============================] - 1s 41ms/step - loss: 174943.0938 - mae: 174943.0938 - val_loss: 172104.2188 - val_mae: 172104.2188\n",
      "Epoch 235/500\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 174528.8906 - mae: 174528.8906 - val_loss: 171670.7031 - val_mae: 171670.7031\n",
      "Epoch 236/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 174080.8281 - mae: 174080.8281 - val_loss: 171189.3125 - val_mae: 171189.3125\n",
      "Epoch 237/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 173605.3906 - mae: 173605.3906 - val_loss: 170735.2500 - val_mae: 170735.2500\n",
      "Epoch 238/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 173212.1094 - mae: 173212.1094 - val_loss: 170362.5625 - val_mae: 170362.5625\n",
      "Epoch 239/500\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 172764.2031 - mae: 172764.2031 - val_loss: 169989.4062 - val_mae: 169989.4062\n",
      "Epoch 240/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 172284.4375 - mae: 172284.4375 - val_loss: 169418.6562 - val_mae: 169418.6562\n",
      "Epoch 241/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 171835.8750 - mae: 171835.8750 - val_loss: 168968.7500 - val_mae: 168968.7500\n",
      "Epoch 242/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 171437.3125 - mae: 171437.3125 - val_loss: 168608.0781 - val_mae: 168608.0781\n",
      "Epoch 243/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 170974.7344 - mae: 170974.7344 - val_loss: 168151.0781 - val_mae: 168151.0781\n",
      "Epoch 244/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 170512.1562 - mae: 170512.1562 - val_loss: 167582.7031 - val_mae: 167582.7031\n",
      "Epoch 245/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 170020.4531 - mae: 170020.4531 - val_loss: 167098.9688 - val_mae: 167098.9688\n",
      "Epoch 246/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 169574.8125 - mae: 169574.8125 - val_loss: 166681.2344 - val_mae: 166681.2344\n",
      "Epoch 247/500\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 169119.4844 - mae: 169119.4844 - val_loss: 166312.1875 - val_mae: 166312.1875\n",
      "Epoch 248/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 168727.7969 - mae: 168727.7969 - val_loss: 166063.8750 - val_mae: 166063.8750\n",
      "Epoch 249/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 168223.4844 - mae: 168223.4844 - val_loss: 165555.5625 - val_mae: 165555.5625\n",
      "Epoch 250/500\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 167784.4844 - mae: 167784.4844 - val_loss: 165019.8281 - val_mae: 165019.8281\n",
      "Epoch 251/500\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 167326.5469 - mae: 167326.5469 - val_loss: 164391.8438 - val_mae: 164391.8438\n",
      "Epoch 252/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 166854.2344 - mae: 166854.2344 - val_loss: 164007.1875 - val_mae: 164007.1875\n",
      "Epoch 253/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 166426.7500 - mae: 166426.7500 - val_loss: 163622.2188 - val_mae: 163622.2188\n",
      "Epoch 254/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 165932.9844 - mae: 165932.9844 - val_loss: 163015.4531 - val_mae: 163015.4531\n",
      "Epoch 255/500\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 165427.7188 - mae: 165427.7188 - val_loss: 162683.0781 - val_mae: 162683.0781\n",
      "Epoch 256/500\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 164959.1875 - mae: 164959.1875 - val_loss: 162275.9844 - val_mae: 162275.9844\n",
      "Epoch 257/500\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 164552.4062 - mae: 164552.4062 - val_loss: 161559.8906 - val_mae: 161559.8906\n",
      "Epoch 258/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 164074.8281 - mae: 164074.8281 - val_loss: 161185.7656 - val_mae: 161185.7656\n",
      "Epoch 259/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 163557.6719 - mae: 163557.6719 - val_loss: 160703.8906 - val_mae: 160703.8906\n",
      "Epoch 260/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 163105.8594 - mae: 163105.8594 - val_loss: 160192.9531 - val_mae: 160192.9531\n",
      "Epoch 261/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 162682.6562 - mae: 162682.6562 - val_loss: 159732.7969 - val_mae: 159732.7969\n",
      "Epoch 262/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 162155.7969 - mae: 162155.7969 - val_loss: 159233.1562 - val_mae: 159233.1562\n",
      "Epoch 263/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 161665.3906 - mae: 161665.3906 - val_loss: 158630.5781 - val_mae: 158630.5781\n",
      "Epoch 264/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 161249.7656 - mae: 161249.7656 - val_loss: 158221.9062 - val_mae: 158221.9062\n",
      "Epoch 265/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 160721.5625 - mae: 160721.5625 - val_loss: 157734.7188 - val_mae: 157734.7188\n",
      "Epoch 266/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 160239.1094 - mae: 160239.1094 - val_loss: 157249.6719 - val_mae: 157249.6719\n",
      "Epoch 267/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 159757.3281 - mae: 159757.3281 - val_loss: 156909.0156 - val_mae: 156909.0156\n",
      "Epoch 268/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 159312.4844 - mae: 159312.4844 - val_loss: 156566.2812 - val_mae: 156566.2812\n",
      "Epoch 269/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 158754.0781 - mae: 158754.0781 - val_loss: 155742.7969 - val_mae: 155742.7969\n",
      "Epoch 270/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 158297.8281 - mae: 158297.8281 - val_loss: 155474.3750 - val_mae: 155474.3750\n",
      "Epoch 271/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 157815.1094 - mae: 157815.1094 - val_loss: 154987.3750 - val_mae: 154987.3750\n",
      "Epoch 272/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 157283.9688 - mae: 157283.9688 - val_loss: 154705.0156 - val_mae: 154705.0156\n",
      "Epoch 273/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 156860.1875 - mae: 156860.1875 - val_loss: 154242.2812 - val_mae: 154242.2812\n",
      "Epoch 274/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 156327.0156 - mae: 156327.0156 - val_loss: 153497.0625 - val_mae: 153497.0625\n",
      "Epoch 275/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 155869.6875 - mae: 155869.6875 - val_loss: 153064.3906 - val_mae: 153064.3906\n",
      "Epoch 276/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 155372.2812 - mae: 155372.2812 - val_loss: 152693.3438 - val_mae: 152693.3438\n",
      "Epoch 277/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 154874.5625 - mae: 154874.5625 - val_loss: 152157.7812 - val_mae: 152157.7812\n",
      "Epoch 278/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 154341.0156 - mae: 154341.0156 - val_loss: 151898.0469 - val_mae: 151898.0469\n",
      "Epoch 279/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 153886.8906 - mae: 153886.8906 - val_loss: 151446.0000 - val_mae: 151446.0000\n",
      "Epoch 280/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 153324.0938 - mae: 153324.0938 - val_loss: 150550.0156 - val_mae: 150550.0156\n",
      "Epoch 281/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 152845.7031 - mae: 152845.7031 - val_loss: 149793.2031 - val_mae: 149793.2031\n",
      "Epoch 282/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 152395.3125 - mae: 152395.3125 - val_loss: 149188.0625 - val_mae: 149188.0625\n",
      "Epoch 283/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 151851.1406 - mae: 151851.1406 - val_loss: 148575.6562 - val_mae: 148575.6562\n",
      "Epoch 284/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 151350.0156 - mae: 151350.0156 - val_loss: 148162.4531 - val_mae: 148162.4531\n",
      "Epoch 285/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 150815.7500 - mae: 150815.7500 - val_loss: 147820.8438 - val_mae: 147820.8438\n",
      "Epoch 286/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 150340.9531 - mae: 150340.9531 - val_loss: 147442.3438 - val_mae: 147442.3438\n",
      "Epoch 287/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 149814.7188 - mae: 149814.7188 - val_loss: 146761.1406 - val_mae: 146761.1406\n",
      "Epoch 288/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 149312.5938 - mae: 149312.5938 - val_loss: 146568.6562 - val_mae: 146568.6719\n",
      "Epoch 289/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 148792.5000 - mae: 148792.5000 - val_loss: 146012.6719 - val_mae: 146012.6719\n",
      "Epoch 290/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 148289.0625 - mae: 148289.0625 - val_loss: 145133.5000 - val_mae: 145133.5000\n",
      "Epoch 291/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 147792.0938 - mae: 147792.0938 - val_loss: 145208.2969 - val_mae: 145208.2969\n",
      "Epoch 292/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 147218.7656 - mae: 147218.7656 - val_loss: 144678.7969 - val_mae: 144678.7969\n",
      "Epoch 293/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 146706.4688 - mae: 146706.4688 - val_loss: 143777.9062 - val_mae: 143777.9062\n",
      "Epoch 294/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 146260.1719 - mae: 146260.1719 - val_loss: 143282.0781 - val_mae: 143282.0781\n",
      "Epoch 295/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 145714.9844 - mae: 145714.9844 - val_loss: 142736.9062 - val_mae: 142736.9062\n",
      "Epoch 296/500\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 145192.1406 - mae: 145192.1406 - val_loss: 142101.5156 - val_mae: 142101.5156\n",
      "Epoch 297/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 144632.7969 - mae: 144632.7969 - val_loss: 141408.3281 - val_mae: 141408.3281\n",
      "Epoch 298/500\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 144130.7344 - mae: 144130.7344 - val_loss: 140866.1719 - val_mae: 140866.1719\n",
      "Epoch 299/500\n",
      "36/36 [==============================] - 1s 39ms/step - loss: 143598.9688 - mae: 143598.9688 - val_loss: 140265.1406 - val_mae: 140265.1406\n",
      "Epoch 300/500\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 143041.6562 - mae: 143041.6562 - val_loss: 139836.7969 - val_mae: 139836.7969\n",
      "Epoch 301/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 142511.3906 - mae: 142511.3906 - val_loss: 139351.1875 - val_mae: 139351.1875\n",
      "Epoch 302/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 142048.6094 - mae: 142048.6094 - val_loss: 139201.3906 - val_mae: 139201.3906\n",
      "Epoch 303/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 141464.9375 - mae: 141464.9375 - val_loss: 138474.3125 - val_mae: 138474.3125\n",
      "Epoch 304/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 140924.2969 - mae: 140924.2969 - val_loss: 137797.1406 - val_mae: 137797.1406\n",
      "Epoch 305/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 140376.3594 - mae: 140376.3594 - val_loss: 137275.8281 - val_mae: 137275.8281\n",
      "Epoch 306/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 139871.3281 - mae: 139871.3281 - val_loss: 136425.7812 - val_mae: 136425.7812\n",
      "Epoch 307/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 139307.6875 - mae: 139307.6875 - val_loss: 135914.6562 - val_mae: 135914.6562\n",
      "Epoch 308/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 138748.9062 - mae: 138748.9062 - val_loss: 135915.3906 - val_mae: 135915.3906\n",
      "Epoch 309/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 138255.7031 - mae: 138255.7031 - val_loss: 135508.5625 - val_mae: 135508.5625\n",
      "Epoch 310/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 137675.9688 - mae: 137675.9688 - val_loss: 134643.3281 - val_mae: 134643.3281\n",
      "Epoch 311/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 137146.0938 - mae: 137146.0938 - val_loss: 134115.1094 - val_mae: 134115.1094\n",
      "Epoch 312/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 136606.1406 - mae: 136606.1406 - val_loss: 133356.2969 - val_mae: 133356.2969\n",
      "Epoch 313/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 136037.4375 - mae: 136037.4375 - val_loss: 132963.0469 - val_mae: 132963.0469\n",
      "Epoch 314/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 135530.8125 - mae: 135530.8125 - val_loss: 132215.8906 - val_mae: 132215.8906\n",
      "Epoch 315/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 135002.7344 - mae: 135002.7344 - val_loss: 131701.0781 - val_mae: 131701.0781\n",
      "Epoch 316/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 134409.9688 - mae: 134409.9688 - val_loss: 130821.6953 - val_mae: 130821.6953\n",
      "Epoch 317/500\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 133909.1250 - mae: 133909.1250 - val_loss: 130592.7656 - val_mae: 130592.7656\n",
      "Epoch 318/500\n",
      "36/36 [==============================] - 2s 42ms/step - loss: 133310.2031 - mae: 133310.2031 - val_loss: 129858.8672 - val_mae: 129858.8672\n",
      "Epoch 319/500\n",
      "36/36 [==============================] - 1s 39ms/step - loss: 132746.9844 - mae: 132746.9844 - val_loss: 129414.0234 - val_mae: 129414.0234\n",
      "Epoch 320/500\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 132226.1875 - mae: 132226.1875 - val_loss: 129216.8203 - val_mae: 129216.8203\n",
      "Epoch 321/500\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 131648.9844 - mae: 131648.9844 - val_loss: 128649.3047 - val_mae: 128649.3047\n",
      "Epoch 322/500\n",
      "36/36 [==============================] - 1s 39ms/step - loss: 131117.0156 - mae: 131117.0156 - val_loss: 128419.2266 - val_mae: 128419.2266\n",
      "Epoch 323/500\n",
      "36/36 [==============================] - 2s 42ms/step - loss: 130507.9375 - mae: 130507.9375 - val_loss: 128000.6328 - val_mae: 128000.6328\n",
      "Epoch 324/500\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 129957.2188 - mae: 129957.2188 - val_loss: 127150.3125 - val_mae: 127150.3125\n",
      "Epoch 325/500\n",
      "36/36 [==============================] - 2s 46ms/step - loss: 129377.6094 - mae: 129377.6094 - val_loss: 126657.6250 - val_mae: 126657.6250\n",
      "Epoch 326/500\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 128812.2578 - mae: 128812.2578 - val_loss: 126061.8438 - val_mae: 126061.8438\n",
      "Epoch 327/500\n",
      "36/36 [==============================] - 2s 62ms/step - loss: 128215.5312 - mae: 128215.5312 - val_loss: 125410.1172 - val_mae: 125410.1172\n",
      "Epoch 328/500\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 127639.6016 - mae: 127639.6016 - val_loss: 124736.6484 - val_mae: 124736.6484\n",
      "Epoch 329/500\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 127206.1953 - mae: 127206.1953 - val_loss: 124280.4375 - val_mae: 124280.4375\n",
      "Epoch 330/500\n",
      "36/36 [==============================] - 2s 52ms/step - loss: 126575.8828 - mae: 126575.8828 - val_loss: 123693.3750 - val_mae: 123693.3750\n",
      "Epoch 331/500\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 125986.5156 - mae: 125986.5156 - val_loss: 122990.0547 - val_mae: 122990.0547\n",
      "Epoch 332/500\n",
      "36/36 [==============================] - 2s 59ms/step - loss: 125403.0625 - mae: 125403.0625 - val_loss: 122827.7500 - val_mae: 122827.7500\n",
      "Epoch 333/500\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 124826.7812 - mae: 124826.7812 - val_loss: 122639.5000 - val_mae: 122639.5000\n",
      "Epoch 334/500\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 124257.6406 - mae: 124257.6406 - val_loss: 121275.3906 - val_mae: 121275.3906\n",
      "Epoch 335/500\n",
      "36/36 [==============================] - 2s 51ms/step - loss: 123724.4688 - mae: 123724.4688 - val_loss: 120767.2188 - val_mae: 120767.2188\n",
      "Epoch 336/500\n",
      "36/36 [==============================] - 2s 47ms/step - loss: 123091.6406 - mae: 123091.6406 - val_loss: 120529.8750 - val_mae: 120529.8750\n",
      "Epoch 337/500\n",
      "36/36 [==============================] - 2s 46ms/step - loss: 122500.3438 - mae: 122500.3438 - val_loss: 118982.6641 - val_mae: 118982.6641\n",
      "Epoch 338/500\n",
      "36/36 [==============================] - 2s 46ms/step - loss: 121938.1016 - mae: 121938.1016 - val_loss: 119802.0312 - val_mae: 119802.0312\n",
      "Epoch 339/500\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 121323.2969 - mae: 121323.2969 - val_loss: 119144.4844 - val_mae: 119144.4844\n",
      "Epoch 340/500\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 120792.0078 - mae: 120792.0078 - val_loss: 118738.0703 - val_mae: 118738.0703\n",
      "Epoch 341/500\n",
      "36/36 [==============================] - 2s 50ms/step - loss: 120193.7656 - mae: 120193.7656 - val_loss: 117644.5469 - val_mae: 117644.5469\n",
      "Epoch 342/500\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 119616.2578 - mae: 119616.2578 - val_loss: 116635.8594 - val_mae: 116635.8594\n",
      "Epoch 343/500\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 119024.3594 - mae: 119024.3594 - val_loss: 116239.0391 - val_mae: 116239.0391\n",
      "Epoch 344/500\n",
      "36/36 [==============================] - 2s 65ms/step - loss: 118453.7578 - mae: 118453.7578 - val_loss: 115931.2266 - val_mae: 115931.2266\n",
      "Epoch 345/500\n",
      "36/36 [==============================] - 2s 54ms/step - loss: 117843.1250 - mae: 117843.1250 - val_loss: 115498.8828 - val_mae: 115498.8828\n",
      "Epoch 346/500\n",
      "36/36 [==============================] - 2s 55ms/step - loss: 117249.1328 - mae: 117249.1328 - val_loss: 114609.1641 - val_mae: 114609.1641\n",
      "Epoch 347/500\n",
      "36/36 [==============================] - 2s 54ms/step - loss: 116604.5547 - mae: 116604.5547 - val_loss: 113064.0938 - val_mae: 113064.0938\n",
      "Epoch 348/500\n",
      "36/36 [==============================] - 2s 54ms/step - loss: 116023.5781 - mae: 116023.5781 - val_loss: 113295.1953 - val_mae: 113295.1953\n",
      "Epoch 349/500\n",
      "36/36 [==============================] - 2s 60ms/step - loss: 115517.6719 - mae: 115517.6719 - val_loss: 112993.1250 - val_mae: 112993.1250\n",
      "Epoch 350/500\n",
      "36/36 [==============================] - 2s 54ms/step - loss: 114807.7031 - mae: 114807.7031 - val_loss: 113017.4219 - val_mae: 113017.4219\n",
      "Epoch 351/500\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 114266.7344 - mae: 114266.7344 - val_loss: 112074.0859 - val_mae: 112074.0859\n",
      "Epoch 352/500\n",
      "36/36 [==============================] - 2s 55ms/step - loss: 113716.2500 - mae: 113716.2500 - val_loss: 111511.7266 - val_mae: 111511.7266\n",
      "Epoch 353/500\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 113129.6016 - mae: 113129.6016 - val_loss: 110719.6094 - val_mae: 110719.6094\n",
      "Epoch 354/500\n",
      "36/36 [==============================] - 2s 53ms/step - loss: 112487.4766 - mae: 112487.4766 - val_loss: 110520.3672 - val_mae: 110520.3672\n",
      "Epoch 355/500\n",
      "36/36 [==============================] - 2s 55ms/step - loss: 111855.7656 - mae: 111855.7656 - val_loss: 109942.3047 - val_mae: 109942.3047\n",
      "Epoch 356/500\n",
      "36/36 [==============================] - 2s 63ms/step - loss: 111213.4766 - mae: 111213.4766 - val_loss: 109265.9219 - val_mae: 109265.9219\n",
      "Epoch 357/500\n",
      "36/36 [==============================] - 2s 55ms/step - loss: 110607.7031 - mae: 110607.7031 - val_loss: 108849.3047 - val_mae: 108849.3047\n",
      "Epoch 358/500\n",
      "36/36 [==============================] - 2s 54ms/step - loss: 110034.5703 - mae: 110034.5703 - val_loss: 108463.1719 - val_mae: 108463.1719\n",
      "Epoch 359/500\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 109480.5234 - mae: 109480.5234 - val_loss: 107851.0156 - val_mae: 107851.0156\n",
      "Epoch 360/500\n",
      "36/36 [==============================] - 2s 61ms/step - loss: 108847.6172 - mae: 108847.6172 - val_loss: 107095.0469 - val_mae: 107095.0469\n",
      "Epoch 361/500\n",
      "36/36 [==============================] - 2s 58ms/step - loss: 108268.8594 - mae: 108268.8594 - val_loss: 106311.2422 - val_mae: 106311.2422\n",
      "Epoch 362/500\n",
      "36/36 [==============================] - 2s 56ms/step - loss: 107566.1016 - mae: 107566.1016 - val_loss: 105747.6641 - val_mae: 105747.6641\n",
      "Epoch 363/500\n",
      "36/36 [==============================] - 2s 55ms/step - loss: 107026.4922 - mae: 107026.4922 - val_loss: 104673.1797 - val_mae: 104673.1797\n",
      "Epoch 364/500\n",
      "36/36 [==============================] - 2s 60ms/step - loss: 106372.9609 - mae: 106372.9609 - val_loss: 104372.7344 - val_mae: 104372.7344\n",
      "Epoch 365/500\n",
      "36/36 [==============================] - 2s 55ms/step - loss: 105864.3750 - mae: 105864.3750 - val_loss: 104015.1328 - val_mae: 104015.1328\n",
      "Epoch 366/500\n",
      "36/36 [==============================] - 2s 70ms/step - loss: 105173.5234 - mae: 105173.5234 - val_loss: 103917.7734 - val_mae: 103917.7734\n",
      "Epoch 367/500\n",
      "36/36 [==============================] - 3s 80ms/step - loss: 104504.7500 - mae: 104504.7500 - val_loss: 102985.1875 - val_mae: 102985.1875\n",
      "Epoch 368/500\n",
      "36/36 [==============================] - 2s 63ms/step - loss: 103896.6875 - mae: 103896.6875 - val_loss: 102704.1484 - val_mae: 102704.1484\n",
      "Epoch 369/500\n",
      "36/36 [==============================] - 3s 74ms/step - loss: 103267.0391 - mae: 103267.0391 - val_loss: 101919.9609 - val_mae: 101919.9609\n",
      "Epoch 370/500\n",
      "36/36 [==============================] - 3s 92ms/step - loss: 102621.8203 - mae: 102621.8203 - val_loss: 101830.0469 - val_mae: 101830.0469\n",
      "Epoch 371/500\n",
      "36/36 [==============================] - 3s 80ms/step - loss: 102077.9531 - mae: 102077.9531 - val_loss: 100109.0234 - val_mae: 100109.0234\n",
      "Epoch 372/500\n",
      "36/36 [==============================] - 2s 70ms/step - loss: 101379.3125 - mae: 101379.3125 - val_loss: 98949.3984 - val_mae: 98949.3984\n",
      "Epoch 373/500\n",
      "36/36 [==============================] - 2s 63ms/step - loss: 100869.3984 - mae: 100869.3984 - val_loss: 98311.4766 - val_mae: 98311.4766\n",
      "Epoch 374/500\n",
      "36/36 [==============================] - 2s 60ms/step - loss: 100263.0391 - mae: 100263.0391 - val_loss: 99751.2812 - val_mae: 99751.2812\n",
      "Epoch 375/500\n",
      "36/36 [==============================] - 2s 69ms/step - loss: 99613.7031 - mae: 99613.7031 - val_loss: 98121.2891 - val_mae: 98121.2891\n",
      "Epoch 376/500\n",
      "36/36 [==============================] - 2s 60ms/step - loss: 98978.7812 - mae: 98978.7812 - val_loss: 97547.3281 - val_mae: 97547.3281\n",
      "Epoch 377/500\n",
      "36/36 [==============================] - 2s 60ms/step - loss: 98287.7031 - mae: 98287.7031 - val_loss: 98332.3672 - val_mae: 98332.3672\n",
      "Epoch 378/500\n",
      "36/36 [==============================] - 2s 70ms/step - loss: 97707.6016 - mae: 97707.6016 - val_loss: 97255.1562 - val_mae: 97255.1562\n",
      "Epoch 379/500\n",
      "36/36 [==============================] - 2s 57ms/step - loss: 97088.8516 - mae: 97088.8516 - val_loss: 96809.0000 - val_mae: 96809.0000\n",
      "Epoch 380/500\n",
      "36/36 [==============================] - 2s 45ms/step - loss: 96454.2031 - mae: 96454.2031 - val_loss: 94778.1406 - val_mae: 94778.1406\n",
      "Epoch 381/500\n",
      "36/36 [==============================] - 2s 42ms/step - loss: 95837.7031 - mae: 95837.7031 - val_loss: 94628.5625 - val_mae: 94628.5625\n",
      "Epoch 382/500\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 95147.1641 - mae: 95147.1641 - val_loss: 95569.1094 - val_mae: 95569.1094\n",
      "Epoch 383/500\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 94443.1250 - mae: 94443.1250 - val_loss: 93367.5156 - val_mae: 93367.5156\n",
      "Epoch 384/500\n",
      "36/36 [==============================] - 2s 47ms/step - loss: 93838.7891 - mae: 93838.7891 - val_loss: 92562.0859 - val_mae: 92562.0859\n",
      "Epoch 385/500\n",
      "36/36 [==============================] - 2s 43ms/step - loss: 93350.9844 - mae: 93350.9844 - val_loss: 94182.7031 - val_mae: 94182.7031\n",
      "Epoch 386/500\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 92589.1016 - mae: 92589.1016 - val_loss: 92647.8594 - val_mae: 92647.8594\n",
      "Epoch 387/500\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 92003.2812 - mae: 92003.2812 - val_loss: 92226.8906 - val_mae: 92226.8906\n",
      "Epoch 388/500\n",
      "36/36 [==============================] - 1s 39ms/step - loss: 91320.8047 - mae: 91320.8047 - val_loss: 91151.7578 - val_mae: 91151.7578\n",
      "Epoch 389/500\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 90707.9141 - mae: 90707.9141 - val_loss: 89847.4062 - val_mae: 89847.4062\n",
      "Epoch 390/500\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 90034.3516 - mae: 90034.3516 - val_loss: 90244.5000 - val_mae: 90244.5000\n",
      "Epoch 391/500\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 89396.6719 - mae: 89396.6719 - val_loss: 89969.5469 - val_mae: 89969.5469\n",
      "Epoch 392/500\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 88758.4766 - mae: 88758.4766 - val_loss: 88351.8750 - val_mae: 88351.8750\n",
      "Epoch 393/500\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 88199.6016 - mae: 88199.6016 - val_loss: 88532.7188 - val_mae: 88532.7188\n",
      "Epoch 394/500\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 87457.7344 - mae: 87457.7344 - val_loss: 88696.8359 - val_mae: 88696.8359\n",
      "Epoch 395/500\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 86879.8125 - mae: 86879.8125 - val_loss: 87066.8438 - val_mae: 87066.8438\n",
      "Epoch 396/500\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 86190.0469 - mae: 86190.0469 - val_loss: 86567.4609 - val_mae: 86567.4609\n",
      "Epoch 397/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 85489.0391 - mae: 85489.0391 - val_loss: 86848.0703 - val_mae: 86848.0703\n",
      "Epoch 398/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 84873.5234 - mae: 84873.5234 - val_loss: 86595.0469 - val_mae: 86595.0469\n",
      "Epoch 399/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 84250.1641 - mae: 84250.1641 - val_loss: 85539.5469 - val_mae: 85539.5469\n",
      "Epoch 400/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 83549.8984 - mae: 83549.8984 - val_loss: 84719.0078 - val_mae: 84719.0078\n",
      "Epoch 401/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 82902.5547 - mae: 82902.5547 - val_loss: 84510.9609 - val_mae: 84510.9609\n",
      "Epoch 402/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 82268.7656 - mae: 82268.7656 - val_loss: 83168.3672 - val_mae: 83168.3672\n",
      "Epoch 403/500\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 81637.0547 - mae: 81637.0547 - val_loss: 82603.7344 - val_mae: 82603.7344\n",
      "Epoch 404/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 80904.2969 - mae: 80904.2969 - val_loss: 84691.6094 - val_mae: 84691.6094\n",
      "Epoch 405/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 80198.8828 - mae: 80198.8828 - val_loss: 81727.7266 - val_mae: 81727.7266\n",
      "Epoch 406/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 79643.5938 - mae: 79643.5938 - val_loss: 80119.5547 - val_mae: 80119.5547\n",
      "Epoch 407/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 79013.5078 - mae: 79013.5078 - val_loss: 81240.2031 - val_mae: 81240.2031\n",
      "Epoch 408/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 78320.5469 - mae: 78320.5469 - val_loss: 80734.1797 - val_mae: 80734.1797\n",
      "Epoch 409/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 77571.9375 - mae: 77571.9375 - val_loss: 80750.4141 - val_mae: 80750.4141\n",
      "Epoch 410/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 77053.2812 - mae: 77053.2812 - val_loss: 80638.9297 - val_mae: 80638.9297\n",
      "Epoch 411/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 76306.8281 - mae: 76306.8281 - val_loss: 80360.8906 - val_mae: 80360.8906\n",
      "Epoch 412/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 75846.1172 - mae: 75846.1172 - val_loss: 78261.4609 - val_mae: 78261.4609\n",
      "Epoch 413/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 75068.1094 - mae: 75068.1094 - val_loss: 78591.5078 - val_mae: 78591.5078\n",
      "Epoch 414/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 74357.7656 - mae: 74357.7656 - val_loss: 77860.2188 - val_mae: 77860.2188\n",
      "Epoch 415/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 73793.7812 - mae: 73793.7812 - val_loss: 76991.6016 - val_mae: 76991.6016\n",
      "Epoch 416/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 73057.3750 - mae: 73057.3750 - val_loss: 77018.2578 - val_mae: 77018.2578\n",
      "Epoch 417/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 72420.2734 - mae: 72420.2734 - val_loss: 77183.0625 - val_mae: 77183.0625\n",
      "Epoch 418/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 71765.5781 - mae: 71765.5781 - val_loss: 73471.4766 - val_mae: 73471.4766\n",
      "Epoch 419/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 71128.4062 - mae: 71128.4062 - val_loss: 76492.5000 - val_mae: 76492.5000\n",
      "Epoch 420/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 70458.8281 - mae: 70458.8281 - val_loss: 73705.6406 - val_mae: 73705.6406\n",
      "Epoch 421/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 69789.0703 - mae: 69789.0703 - val_loss: 72838.7734 - val_mae: 72838.7734\n",
      "Epoch 422/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 69054.6797 - mae: 69054.6797 - val_loss: 73635.0469 - val_mae: 73635.0469\n",
      "Epoch 423/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 68462.8828 - mae: 68462.8828 - val_loss: 72275.7500 - val_mae: 72275.7500\n",
      "Epoch 424/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 67766.8906 - mae: 67766.8906 - val_loss: 73248.8125 - val_mae: 73248.8125\n",
      "Epoch 425/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 67164.9141 - mae: 67164.9141 - val_loss: 73316.1172 - val_mae: 73316.1172\n",
      "Epoch 426/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 66407.3203 - mae: 66407.3203 - val_loss: 70090.2578 - val_mae: 70090.2578\n",
      "Epoch 427/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 65852.5391 - mae: 65852.5391 - val_loss: 71740.6875 - val_mae: 71740.6875\n",
      "Epoch 428/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 65013.5156 - mae: 65013.5156 - val_loss: 70550.2656 - val_mae: 70550.2656\n",
      "Epoch 429/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 64576.2188 - mae: 64576.2188 - val_loss: 71495.1719 - val_mae: 71495.1719\n",
      "Epoch 430/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 63805.5742 - mae: 63805.5742 - val_loss: 69840.0312 - val_mae: 69840.0312\n",
      "Epoch 431/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 63204.5664 - mae: 63204.5664 - val_loss: 67978.9297 - val_mae: 67978.9297\n",
      "Epoch 432/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 62415.0156 - mae: 62415.0156 - val_loss: 70961.3125 - val_mae: 70961.3125\n",
      "Epoch 433/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 61831.9805 - mae: 61831.9805 - val_loss: 68672.4766 - val_mae: 68672.4766\n",
      "Epoch 434/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 61211.1719 - mae: 61211.1719 - val_loss: 69167.9922 - val_mae: 69167.9922\n",
      "Epoch 435/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 60617.1367 - mae: 60617.1367 - val_loss: 66973.4141 - val_mae: 66973.4141\n",
      "Epoch 436/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 59943.9180 - mae: 59943.9180 - val_loss: 68455.7031 - val_mae: 68455.7031\n",
      "Epoch 437/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 59200.6523 - mae: 59200.6523 - val_loss: 67638.2266 - val_mae: 67638.2266\n",
      "Epoch 438/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 58535.7266 - mae: 58535.7266 - val_loss: 66769.7969 - val_mae: 66769.7969\n",
      "Epoch 439/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 57941.1602 - mae: 57941.1602 - val_loss: 65504.9180 - val_mae: 65504.9180\n",
      "Epoch 440/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 57329.0039 - mae: 57329.0039 - val_loss: 65852.3828 - val_mae: 65852.3828\n",
      "Epoch 441/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 56734.7422 - mae: 56734.7422 - val_loss: 63391.9844 - val_mae: 63391.9844\n",
      "Epoch 442/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 55993.7734 - mae: 55993.7734 - val_loss: 64247.5430 - val_mae: 64247.5430\n",
      "Epoch 443/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 55324.5547 - mae: 55324.5547 - val_loss: 63033.3750 - val_mae: 63033.3750\n",
      "Epoch 444/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 54701.7695 - mae: 54701.7695 - val_loss: 61627.0312 - val_mae: 61627.0312\n",
      "Epoch 445/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 54152.8555 - mae: 54152.8555 - val_loss: 63392.4297 - val_mae: 63392.4297\n",
      "Epoch 446/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 53428.0391 - mae: 53428.0391 - val_loss: 63911.5703 - val_mae: 63911.5703\n",
      "Epoch 447/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 52778.5117 - mae: 52778.5117 - val_loss: 61658.4727 - val_mae: 61658.4727\n",
      "Epoch 448/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 52246.2773 - mae: 52246.2773 - val_loss: 61568.8711 - val_mae: 61568.8711\n",
      "Epoch 449/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 51626.1797 - mae: 51626.1797 - val_loss: 62665.2109 - val_mae: 62665.2109\n",
      "Epoch 450/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 50827.0156 - mae: 50827.0156 - val_loss: 60860.9609 - val_mae: 60860.9609\n",
      "Epoch 451/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 50404.2500 - mae: 50404.2500 - val_loss: 61893.3789 - val_mae: 61893.3789\n",
      "Epoch 452/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 49816.0859 - mae: 49816.0859 - val_loss: 59263.5742 - val_mae: 59263.5742\n",
      "Epoch 453/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 49142.1719 - mae: 49142.1719 - val_loss: 61479.8125 - val_mae: 61479.8125\n",
      "Epoch 454/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 48473.5273 - mae: 48473.5273 - val_loss: 59076.0664 - val_mae: 59076.0664\n",
      "Epoch 455/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 47950.1836 - mae: 47950.1836 - val_loss: 60026.0820 - val_mae: 60026.0820\n",
      "Epoch 456/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 47428.1328 - mae: 47428.1328 - val_loss: 58442.5039 - val_mae: 58442.5039\n",
      "Epoch 457/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 46658.3828 - mae: 46658.3828 - val_loss: 58794.1250 - val_mae: 58794.1250\n",
      "Epoch 458/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 46228.4531 - mae: 46228.4531 - val_loss: 58622.8633 - val_mae: 58622.8633\n",
      "Epoch 459/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 45408.0508 - mae: 45408.0508 - val_loss: 58767.8164 - val_mae: 58767.8164\n",
      "Epoch 460/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 44960.3203 - mae: 44960.3203 - val_loss: 58283.6680 - val_mae: 58283.6680\n",
      "Epoch 461/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 44448.6953 - mae: 44448.6953 - val_loss: 58628.2930 - val_mae: 58628.2930\n",
      "Epoch 462/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 43766.1875 - mae: 43766.1875 - val_loss: 55799.4453 - val_mae: 55799.4453\n",
      "Epoch 463/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 43190.8047 - mae: 43190.8047 - val_loss: 57194.7578 - val_mae: 57194.7578\n",
      "Epoch 464/500\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 42700.1406 - mae: 42700.1406 - val_loss: 58755.0938 - val_mae: 58755.0938\n",
      "Epoch 465/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 42077.3477 - mae: 42077.3477 - val_loss: 55948.6211 - val_mae: 55948.6211\n",
      "Epoch 466/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 41517.4023 - mae: 41517.4023 - val_loss: 55785.1562 - val_mae: 55785.1562\n",
      "Epoch 467/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 40869.6836 - mae: 40869.6836 - val_loss: 55846.6875 - val_mae: 55846.6875\n",
      "Epoch 468/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 40212.7227 - mae: 40212.7227 - val_loss: 53704.4023 - val_mae: 53704.4023\n",
      "Epoch 469/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 39689.4727 - mae: 39689.4727 - val_loss: 54556.5781 - val_mae: 54556.5781\n",
      "Epoch 470/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 39086.0859 - mae: 39086.0859 - val_loss: 53498.4141 - val_mae: 53498.4141\n",
      "Epoch 471/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 38633.4727 - mae: 38633.4727 - val_loss: 53886.9922 - val_mae: 53886.9922\n",
      "Epoch 472/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 38212.8516 - mae: 38212.8516 - val_loss: 53232.6328 - val_mae: 53232.6328\n",
      "Epoch 473/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 37686.3984 - mae: 37686.3984 - val_loss: 54582.9805 - val_mae: 54582.9805\n",
      "Epoch 474/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 37109.9492 - mae: 37109.9492 - val_loss: 51826.1406 - val_mae: 51826.1406\n",
      "Epoch 475/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 36712.1211 - mae: 36712.1211 - val_loss: 52452.6406 - val_mae: 52452.6406\n",
      "Epoch 476/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 35909.5508 - mae: 35909.5508 - val_loss: 54393.1484 - val_mae: 54393.1484\n",
      "Epoch 477/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 35481.8828 - mae: 35481.8828 - val_loss: 53863.5781 - val_mae: 53863.5781\n",
      "Epoch 478/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 35036.0234 - mae: 35036.0234 - val_loss: 52292.4414 - val_mae: 52292.4414\n",
      "Epoch 479/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 34419.6406 - mae: 34419.6406 - val_loss: 50951.0469 - val_mae: 50951.0469\n",
      "Epoch 480/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 33924.3594 - mae: 33924.3594 - val_loss: 52414.3281 - val_mae: 52414.3281\n",
      "Epoch 481/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 33502.2695 - mae: 33502.2695 - val_loss: 51389.9492 - val_mae: 51389.9492\n",
      "Epoch 482/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 33072.8945 - mae: 33072.8945 - val_loss: 50324.8828 - val_mae: 50324.8828\n",
      "Epoch 483/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 32364.7520 - mae: 32364.7520 - val_loss: 49662.9062 - val_mae: 49662.9062\n",
      "Epoch 484/500\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 32091.3340 - mae: 32091.3340 - val_loss: 50835.2188 - val_mae: 50835.2188\n",
      "Epoch 485/500\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 31600.6504 - mae: 31600.6504 - val_loss: 50789.5625 - val_mae: 50789.5625\n",
      "Epoch 486/500\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 31119.3281 - mae: 31119.3281 - val_loss: 49540.8203 - val_mae: 49540.8203\n",
      "Epoch 487/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 30589.9219 - mae: 30589.9219 - val_loss: 49890.3242 - val_mae: 49890.3242\n",
      "Epoch 488/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 30546.4219 - mae: 30546.4219 - val_loss: 49978.0664 - val_mae: 49978.0664\n",
      "Epoch 489/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 29775.3750 - mae: 29775.3750 - val_loss: 50042.6562 - val_mae: 50042.6562\n",
      "Epoch 490/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 29475.2715 - mae: 29475.2715 - val_loss: 47844.9102 - val_mae: 47844.9102\n",
      "Epoch 491/500\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 28935.0254 - mae: 28935.0254 - val_loss: 51585.7383 - val_mae: 51585.7383\n",
      "Epoch 492/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 28650.4375 - mae: 28650.4375 - val_loss: 48865.4297 - val_mae: 48865.4297\n",
      "Epoch 493/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 27986.0938 - mae: 27986.0938 - val_loss: 48624.2383 - val_mae: 48624.2383\n",
      "Epoch 494/500\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 27624.0977 - mae: 27624.0977 - val_loss: 48888.4180 - val_mae: 48888.4180\n",
      "Epoch 495/500\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 27328.2109 - mae: 27328.2109 - val_loss: 49286.9648 - val_mae: 49286.9648\n",
      "Epoch 496/500\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 27302.4141 - mae: 27302.4141 - val_loss: 47344.3398 - val_mae: 47344.3398\n",
      "Epoch 497/500\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 26457.9531 - mae: 26457.9531 - val_loss: 45851.5898 - val_mae: 45851.5898\n",
      "Epoch 498/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 26305.7227 - mae: 26305.7227 - val_loss: 48814.7539 - val_mae: 48814.7539\n",
      "Epoch 499/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 25746.8008 - mae: 25746.8008 - val_loss: 45823.9414 - val_mae: 45823.9414\n",
      "Epoch 500/500\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 25790.5664 - mae: 25790.5664 - val_loss: 46348.9102 - val_mae: 46348.9102\n",
      "Minimum validation loss: 45823.94140625\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.BatchNormalization(input_shape=[input_shape]),\n",
    "\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(1),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    "    metrics=['r_square']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=256,\n",
    "    epochs=500,\n",
    "    callbacks=[early_stopping], # put your callbacks in a list\n",
    "    # verbose=0,  # turn off training log\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
    "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 2ms/step\n",
      "number of good predictions for Sequential = 1682347\n",
      "which is 74341.44940344675%\n"
     ]
    }
   ],
   "source": [
    "print_percent_of_good_predictions([model], X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 3ms/step\n",
      "number of good predictions for Sequential = 1682347\n",
      "which is 74341.44940344675%\n"
     ]
    }
   ],
   "source": [
    "error = NUM_OF_HOURS * 60 * 60\n",
    "predictions = model.predict(X_test)\n",
    "predictions_time_diff = np.abs(y_test - predictions)\n",
    "num_of_good_predictions = (predictions_time_diff < error).sum()\n",
    "percent_of_good_predictions = num_of_good_predictions / len(predictions_time_diff)\n",
    "print(f'number of good predictions for {type(model).__name__} = {num_of_good_predictions}')\n",
    "print(f'which is {percent_of_good_predictions * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6952.125   ,  47992.875   ,  23022.875   , ...,  43279.875   ,\n",
       "         43313.875   ,  58036.125   ],\n",
       "       [ 51245.109375,   3699.890625,  21270.109375, ...,   1013.109375,\n",
       "           979.109375, 102329.109375],\n",
       "       [ 29746.53125 ,  25198.46875 ,    228.46875 , ...,  20485.46875 ,\n",
       "         20519.46875 ,  80830.53125 ],\n",
       "       ...,\n",
       "       [ 71209.609375,  16264.609375,  41234.609375, ...,  20977.609375,\n",
       "         20943.609375, 122293.609375],\n",
       "       [ 52511.1875  , 107456.1875  ,  82486.1875  , ..., 102743.1875  ,\n",
       "        102777.1875  ,   1427.1875  ],\n",
       "       [   946.15625 ,  55891.15625 ,  30921.15625 , ...,  51178.15625 ,\n",
       "         51212.15625 ,  50137.84375 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions_time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2263, 2263)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions_time_diff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>purchase_datetime_delta</th>\n",
       "      <th>516</th>\n",
       "      <th>620</th>\n",
       "      <th>Kraków</th>\n",
       "      <th>Poznań</th>\n",
       "      <th>Radom</th>\n",
       "      <th>Szczecin</th>\n",
       "      <th>Warszawa</th>\n",
       "      <th>...</th>\n",
       "      <th>1627</th>\n",
       "      <th>1628</th>\n",
       "      <th>1629</th>\n",
       "      <th>1630</th>\n",
       "      <th>1631</th>\n",
       "      <th>1632</th>\n",
       "      <th>1633</th>\n",
       "      <th>1634</th>\n",
       "      <th>1635</th>\n",
       "      <th>1653</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10825</th>\n",
       "      <td>0.013589</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.660933</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>0.027859</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.558801</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>0.119751</td>\n",
       "      <td>0.013733</td>\n",
       "      <td>0.913933</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>0.124076</td>\n",
       "      <td>0.226667</td>\n",
       "      <td>0.659243</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>0.134886</td>\n",
       "      <td>0.023667</td>\n",
       "      <td>0.498738</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6667</th>\n",
       "      <td>0.032173</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>0.177023</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.563015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8928</th>\n",
       "      <td>0.836941</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.762100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8521</th>\n",
       "      <td>0.092714</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.213929</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5647</th>\n",
       "      <td>0.719751</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.903028</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2263 rows × 594 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          price  weight_kg  purchase_datetime_delta  516  620  Kraków  Poznań  \\\n",
       "10825  0.013589   0.000467                 0.660933    0    0       0       0   \n",
       "3593   0.027859   0.001333                 0.558801    0    0       0       0   \n",
       "5653   0.119751   0.013733                 0.913933    1    0       0       0   \n",
       "4854   0.124076   0.226667                 0.659243    0    0       0       1   \n",
       "3864   0.134886   0.023667                 0.498738    0    1       0       0   \n",
       "...         ...        ...                      ...  ...  ...     ...     ...   \n",
       "6667   0.032173   0.008667                 0.177023    1    0       0       0   \n",
       "721    0.632184   0.680000                 0.563015    0    0       1       0   \n",
       "8928   0.836941   0.001733                 0.762100    0    0       0       1   \n",
       "8521   0.092714   0.166667                 0.213929    1    0       0       0   \n",
       "5647   0.719751   0.166667                 0.903028    0    1       0       0   \n",
       "\n",
       "       Radom  Szczecin  Warszawa  ...  1627  1628  1629  1630  1631  1632  \\\n",
       "10825      1         0         0  ...     0     0     0     0     0     0   \n",
       "3593       0         0         0  ...     0     0     0     0     0     0   \n",
       "5653       0         0         0  ...     0     0     0     0     0     0   \n",
       "4854       0         0         0  ...     0     0     0     0     0     0   \n",
       "3864       1         0         0  ...     0     0     0     0     0     0   \n",
       "...      ...       ...       ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "6667       0         1         0  ...     0     0     0     0     0     0   \n",
       "721        0         0         0  ...     0     0     0     0     0     0   \n",
       "8928       0         0         0  ...     0     0     0     0     0     0   \n",
       "8521       0         0         0  ...     0     0     0     0     0     0   \n",
       "5647       0         0         0  ...     0     0     0     0     0     0   \n",
       "\n",
       "       1633  1634  1635  1653  \n",
       "10825     0     0     0     0  \n",
       "3593      0     0     0     0  \n",
       "5653      0     0     0     0  \n",
       "4854      0     0     0     0  \n",
       "3864      0     0     0     0  \n",
       "...     ...   ...   ...   ...  \n",
       "6667      0     0     0     0  \n",
       "721       0     0     0     0  \n",
       "8928      0     0     0     0  \n",
       "8521      0     0     0     0  \n",
       "5647      0     0     0     0  \n",
       "\n",
       "[2263 rows x 594 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>Sequential prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225013.0</td>\n",
       "      <td>218060.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170068.0</td>\n",
       "      <td>173767.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195038.0</td>\n",
       "      <td>195266.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>279415.0</td>\n",
       "      <td>205839.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>331221.0</td>\n",
       "      <td>260865.640625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_test  Sequential prediction\n",
       "0  225013.0          218060.875000\n",
       "1  170068.0          173767.890625\n",
       "2  195038.0          195266.468750\n",
       "3  279415.0          205839.546875\n",
       "4  331221.0          260865.640625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2263 entries, 0 to 2262\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   y_test                 2263 non-null   float64\n",
      " 1   Sequential prediction  2263 non-null   float32\n",
      "dtypes: float32(1), float64(1)\n",
      "memory usage: 26.6 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>Sequential prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2263.000000</td>\n",
       "      <td>2263.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>231077.817941</td>\n",
       "      <td>199851.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>79705.119199</td>\n",
       "      <td>59679.675781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>33091.000000</td>\n",
       "      <td>6380.562012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>174344.000000</td>\n",
       "      <td>151710.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>222482.000000</td>\n",
       "      <td>200075.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>281067.000000</td>\n",
       "      <td>245646.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>535390.000000</td>\n",
       "      <td>354304.312500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              y_test  Sequential prediction\n",
       "count    2263.000000            2263.000000\n",
       "mean   231077.817941          199851.406250\n",
       "std     79705.119199           59679.675781\n",
       "min     33091.000000            6380.562012\n",
       "25%    174344.000000          151710.859375\n",
       "50%    222482.000000          200075.703125\n",
       "75%    281067.000000          245646.937500\n",
       "max    535390.000000          354304.312500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_df = create_df_with_predictions([model], X_test, y_test)\n",
    "display_predictions(y_pred_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
